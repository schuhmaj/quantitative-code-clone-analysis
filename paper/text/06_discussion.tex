% !TeX spellcheck = en_US
\section{Discussion}
\label{sec:discussion}

\autoref{sec:results} has shown that the in \autoref{sec:study_design} presented approach for quantitatively measuring clone coverage in different programming languages is capable of delivering the foundation for further discussion. This section will interpret and explain the observed results and finally evaluate the used approach.

\subsection{Interpretation}

Since pure observation, as already utilized in \autoref{sec:results}, is not sufficient for answering the \aclp{rq} precisely, the two-sample Welch's $t$-test is used to determine if the differences in clone coverage mean values are significant. This test assumes that the sample mean values are distributed normally, which holds due to the Central Limit Theorem \cite{shafer2022introductory}. In contrast to Student's $t$-test, it allows unequal variances, which exist here.
\autoref{tab:stat_test} shows the calculated $t$ statistics and $p$ values for four scenarios. With a significance level of $\alpha = 5\%$, \texttt{C} and \texttt{C/C++} mean values are not significantly different since $p > \alpha$ leads to acceptance of $H_0$: equal means. This circumstance answers \refrq{question:compare_c_cpp} with no.
A Kolmogorovâ€“Smirnov test with an $\alpha=5\%$ comparing the \texttt{C} and \texttt{C/C++} clone coverage distributions leads to the acceptance of the null hypothesis that both distributions are equal. This certitude means that if one would not know which distribution belongs to which language, one could not even determine it.

The subsequent two rows answer \refrq{question:compare_rust_c} and \refrq{question:compare_kotlin_java}. The mean values are significantly different in both cases, leading to rejection of $H_0$ and acceptance of $H_1$: \texttt{C/C++} and \texttt{Java} have, on average, a significantly higher mean clone coverage than respectively \texttt{Rust} and \texttt{Kotlin}. So both questions can be answered with ``yes''.
Lastly, \texttt{C/C++} and \texttt{Java} are compared with the result of no significant difference in the mean values for $\alpha = 5\%$.

\begin{table}[tbh!]
	\centering
	\begin{tabular}{|cc||c|c||c|c|}
		\hline
		\multicolumn{2}{|c||}{Samples} & \multicolumn{2}{c||}{$SLOC > 0$} & \multicolumn{2}{c|}{$SLOC \geq 1000000$}  \\
		\multicolumn{2}{|c||}{from} & \multicolumn{1}{c}{$t$ statistic} & \multicolumn{1}{c||}{$p$ value} & \multicolumn{1}{c}{$t$ statistic} & $p$ value \\
		\hline
		\hline
		C & C/C++ & $-1.021$ & $0.846$ & $1.105$ & $0.135$ \\
		\hline
		\hline
		C/C++ & Rust & $9.936$ & $<.001$ & $6.612$ & $<.001$ \\
		\hline
		Java & Kotlin & $5.946$ & $<.001$ & $2.500$ & $0.006$ \\
		\hline
		\hline
		C/C++ & Java & $0.720$ & $0.236$ & $0.643$ & $0.260$ \\
		\hline
	\end{tabular}
	\caption{Results of Welch's $t$-test with $H_0$ being that the means of the two underlying distributions have equal means and $H_1$ that the mean of the first distribution is greater than the mean of the second sample.}
	\label{tab:stat_test}
\end{table}

\begin{figure}[tbh!]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{figures/comparison/comparison_all_5percent}
		\caption{with minimal $SLOC > 0$}
		\label{fig:matrix_comp_all}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{figures/comparison/comparison_million_5percent}
		\caption{with minimal $SLOC \geq 10^{6}$}
		\label{fig:matrix_comp_million}
	\end{subfigure}
	\caption{Matrix comparison which programming languages have significantly lower mean clone coverage (green), equal mean clone coverage (yellow), and higher mean clone coverage (red) with an $\alpha=5\%$. The comparison is performed by using Welch's $t$-tests and Permutation Testing, both yielding the same results. The matrix is read as follows: ``\textit{Language 1} performs better/neutral/worse than \textit{Language 2}''.}
	\label{fig:matrix_comp}
\end{figure}

\autoref{fig:matrix_comp} lists all possible pairings of examined programming languages and depicts which one performs significantly better than its companion. It complements \refrq{question:comparing_age} by answering the top-level \ac{rq} and gives an unambiguous response for \refrq{question:clone_coverage_lowest}.
\autoref{fig:matrix_comp} clearly shows that \texttt{Python} always performs better in terms of mean code clone coverage than any other programming language independent of the applied filter for \aclp{sloc}. So \refrq{question:clone_coverage_lowest}'s answer is \texttt{Python}, which was first released in 1991\footnote{\url{https://en.wikipedia.org/wiki/Python_(programming_language)}, lasted accessed: 13.07.2022}.
This fact also implies that newer generation programming languages do not necessarily have lower code clone coverage than older programming languages. The fact that \texttt{Go} behaves in terms of mean code clone coverage similarly to \texttt{C/C++} and \texttt{Java} supports this circumstance, even if it first appeared in 2009\footnote{\url{https://en.wikipedia.org/wiki/Go_(programming_language)}, last accessed: 13.07.2022}.

In consequence, the answer to top-level \refrq{question:comparing_age} is: ``it depends'' on the concrete context, as the direct competitors, \texttt{Rust} and \texttt{Kotlin}, perform better than \texttt{C/C++} and \texttt{Java}. In contrast, the study yields a very different result when examining \texttt{Go} and \texttt{Python}. To conclude, it is impossible to generalize \refrq{question:comparing_age} for every scenario to a straightforward solution.

\subsection{Implications of the result}

%Can we infer that a simpler, less verbose, more modern programming language always leads to less code clones?

At the outset, the extracted data does not imply that a particular programming language, e.g., \texttt{Python}, which performed best, should be used for every project. 
However, the results show that specific languages, \texttt{Python}, \texttt{Kotlin}, and \texttt{Rust}, performed significantly better than \texttt{C/C++} or \texttt{Java} and, therefore, seem to have advantages over the others.
A posteriori, one reason for fewer clones could be a less verbose, more straightforward programming language design, as discussed in \autoref{sec:similiar_analyses}.
Yet this argument can be partially discarded since \texttt{Go} performs for projects bigger than $10^6$ \ac{sloc}, similarly to \texttt{Java} and \texttt{C/C++}, although \texttt{Go} has the smallest set of keywords - implying simplicity, as depicted in \autoref{tab:keyword_number}.

\begin{table}[tbh!]
	\centering
	\begin{tabular}{|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{1.5cm}|}
		\hline
		C/C++ 17 & Kotlin 1.4 & Rust 1.46 & Java 14 & C18 & Python 3.8 & Go 1.15 \\
		\hline
		84 & 79 & 53 & 51 & 44 & 35 & 25 \\
		\hline
	\end{tabular}
	\caption{Number of keywords for each examined programming language \cite{meyer2022keywords}}
	\label{tab:keyword_number}
\end{table}

Further, \autoref{sec:similiar_analyses} already introduced build systems as crucial for project success. Here, another reason might be located. Simple to use package managers make importing existing solutions much more effortless, thereby preventing \textit{Reinventing the wheel}, a.k.a copying existing solutions. \texttt{Rust} and \texttt{Python} come with package managers \texttt{cargo} and \texttt{pip}, making the use of existing solutions much more manageable. In comparison, \texttt{C/C++} and \texttt{Java} rely on build systems like \texttt{CMake} and \texttt{Maven}, usually with a high learning curve. For example, to include a simple package or library in \texttt{Python}, one must install and import it, whereas, in \texttt{CMake}, far more effort would be needed.

Lastly, one does not forget that, e.g., \texttt{Kotlin} and \texttt{Rust} improve \texttt{Java} and \texttt{C/C++}, respectively introducing null-safety and ownership. Thus these improvements avoid redundant, often repeated runtime checks like mentioned in \autoref{sec:reasons_for_code_clones} (reusing certain code structures).

Despite these reasons, the difference in mean clone coverage does not necessarily indicate that the programming language has flaws.
Older projects often have a larger (legacy) code base, which already can contain clones and that makes it easy to copy and paste from there. 
However, \texttt{Rust}, \texttt{Kotlin}, and \texttt{Go} projects cannot be as old as projects in \texttt{C/C++} or \texttt{Java}. Consequently, the newer programming languages have some ``time'' advantage over the older ones, with projects having fewer developer changes and less legacy code.

In conclusion, we can finalize with the strong empirical suspect, but with no absolute certainty, that \texttt{Python}, \texttt{Kotlin}, and \texttt{Rust} are better designed to prevent code cloning than \texttt{C/C++}, \texttt{Java}, and \texttt{Go}, especially for larger projects with more extensive code bases.

 
\subsection{Threats to validity}

First, the \textit{awesome lists} are different in size and contain different content for every programming language. Numerous contributors created them. Nonetheless, we have little information about why exactly something is on the list. So it is likely, that each list is subject to a certain bias. Additionally, the lists can also incorporate repositories containing only code snippets and similar utilities. The expressive analysis for more than $10^6$ \ac{sloc} should ensure that such projects are excluded, but a residual risk remains.

Directly related to this, the programming projects originate all from \textit{GitHub} and are publicly available. This circumstance is definitely a bias, since our study is not able to cover proprietary code nor big projects hosted on a different source.

The most considerable bias could be the evolution of programming languages. Writing modern \texttt{C++20} follows different guidelines than writing \texttt{C++98}. This behaves analogously to \texttt{Java} and \texttt{Python}. However, the study does not include when the projects were initially written and if they were refactored.

Lastly, $5224$ projects of only seven different programming languages were analyzed, which is, of course, only a tiny fraction at all. Moreover, the toolchain illustrated in \autoref{sec:study_design} could be inaccurate due to bugs in the Python script or \teamscale{}'s code clone detection, despite the strive for the best.

\subsection{Future Work}

Directly related to the previous lessons of this section, future work could improve the solution by extending the scale of investigated projects and the concrete source. For example, a discarded approach for this work was the inclusion of projects from \textit{GitHub} with a certain amount of stars. The inclusion of projects from sources other than \textit{GitHub} or actually including examples from, e.g., a company with projects in different programming languages would also be of interest.

Further, independent validation of these results with another toolchain or another code clone detector than \teamscale{} is also thinkable.

A more qualitative approach could also be undertaken. Here, one could analyze only specific projects in more detail, like logging frameworks which usually exist in nearly every programming language and have similar functionality.
 
